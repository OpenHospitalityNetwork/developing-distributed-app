# 2023-05-09

## The Struggle with Querying

In one of the previous articles, we wrote about [querying RDF graph with shapes](../../season-1/my-profile.md#better-way).

Since then, we also learned about [SHEX shapes](https://shex.io/), and started using them with a new library [LDO](https://www.npmjs.com/package/ldo) ([repository](https://github.com/o-development/ldo)).

### LDO

It's a brilliant library. You just need to provide shapes for the data your app expects. Based on these shapes, you can then fetch some rdf datasets, then query and update them like normal TypeScript objects. We deliberately say TypeScript and not JavaScript, because these objects are well typed. The library is doing its magic using [Proxies](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Proxy).

### Issues with querying

We also try to rethink the way we query data.

Until now, we were using rtk-query with Comunica, LDO or other ways. The whole process of fetching or saving the data over several files was done in a single method. And the caching was done on the level of rtk-query. On one hand, this lead to re-fetching many files unnecessarily[^re-fetching], on the other hand, Comunica was getting stuck because of its internal caching system[^comunica-caching]. In traversal mode, Comunica was also trying to fetch all files it could encounter. Also, we had to wait for all results to be found before returning them.

[^re-fetching]: Different queries often need to download the same files to get to the needed data. When caching is done on the level of query (not files), rtk-query doesn't know it can reuse these requests, so it sends them repeatedly. So everything takes longer.

[^comunica-caching]: It's entirely possible that we weren't using Comunica properly, and its caching system works fine. However, we haven't experienced that.

So, things were taking a long time.

### Progressive querying

We try out a new way - with react-query and LDO. With react-query, we fetch the files and cache them on file (request) level.

Then, as soon as we have fetched data &mdash; with hooks and LDO &mdash; we figure out the next files to fetch and query, and desired data to return. We don't cache results on this level.[^caching-local-work]

[^caching-local-work]: We cache the fetching work, and don't cache the work of querying fetched data locally. We assume that network requests take significantly more time than local javascript. We may need to verify this assumption.

With this architecture, the partial results are shown as soon as they're available. Also, when a specific file gets updated and invalidated, only the minimum necessary requests get repeated in order to update the results. We call this _incremental_ or _progressive querying_.

It's been tough to implement it. [First attempt](https://github.com/OpenHospitalityNetwork/sleepy-bike-solid/commit/50fcd89b8d0f096c7c0e8b517061fd0445d43d46) was verbose, time-consuming and error-prone. It was a lot of code to write. But response time was greatly improved. So it generally sounds like a move in the right direction.

Next, we tried to figure out some query language to add abstraction to this new way of querying. But this was still too hard to write, and we had to take care of both fetching and querying the data.

### Troubles with querying Solid

And this is precisely the difficulty with querying Solid Pods. Unfortunately, Solid Pods are a [bunch of documents](https://ruben.verborgh.org/blog/2022/12/30/lets-talk-about-pods/) with RDF, not a graph. I mean, the graph is there. But it's scattered across the files, and not easy to query.

Perhaps we want to read messages with a specific user. We need to go through personal profile document, extended profile documents, private type index, long chats, find only those chats with the specific user (only) and their chat with us, then search through long chat folders, and finally retrieve messages from there. We also need to locate inbox and find new received messages there.

So we don't just match shapes. We must locate and fetch files with additional data, and ignore links that we don't need.

### Further ideas.

We can fetch the whole relevant dataset, then query it. With LDO. We wouldn't explicitly have to say which files to fetch. If they stand in the way, we can fetch them and add to the dataset.

We are playing with this idea now. But it's not easy to figure out such query language. It also feels like reinventing the wheel...

### Example

To fetch messages with a specific user:

- start with my webId
- find extended profile documents with `webId = rdfs:seeAlso => ?extendedProfile`
- find private type index by going `webId = solid:privateTypeIndex => ?privateTypeIndex`
- in private type index, find locations of `meeting:LongChat`
- in chat, find participations
- filter only chats which have the other user as participant (and only the other user)
- find other chat referenced by the other participant
- go to container of both chats
- in both chat containers, go through folders with years, months and days, and collect files with messages in them
- display the messages

And that's a lot of steps. And i'm leaning towards executing them in sequence. Not coming up with a query language like SPARQL which matches patterns instead.

